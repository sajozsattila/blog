{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c55d4b2-afc7-4c37-84b6-2f9715c10f81",
   "metadata": {},
   "source": [
    "# Set up Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5649ed-f1d6-4da6-8557-647979b4cd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔭 OpenTelemetry Tracing Details 🔭\n",
      "|  Phoenix Project: Basic_RAG\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'api_key': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ⚠️ WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "import os\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "\n",
    "project_name = \"Basic_RAG\"\n",
    "\n",
    "# Add Phoenix API Key for tracing\n",
    "phoenix_key = ''\n",
    "with open('phoenix_key.txt', 'r') as file:\n",
    "    phoenix_key = file.read()\n",
    "os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={phoenix_key}\"\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={phoenix_key}\";\n",
    "os.environ['PHOENIX_PROJECT_NAME'] = project_name\n",
    "\n",
    "# configure the Phoenix tracer\n",
    "tracer_provider = register(\n",
    "  project_name=project_name, # Default is 'default'\n",
    "  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n",
    ")\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider = tracer_provider)\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ba17e-a050-4d64-8169-2c4abf63950c",
   "metadata": {},
   "source": [
    "# Basic imports and setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53cea51-9724-48ca-869c-cc3214c10792",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n",
      "W0901 09:42:50.029000 55652 site-packages/torch/distributed/elastic/multiprocessing/redirects.py:29] NOTE: Redirects are currently not supported in Windows or MacOs.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from time import sleep\n",
    "from pybars import Compiler\n",
    "import yaml\n",
    "from typing import Callable\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# import own utility functions\n",
    "from pdf_preprocessing import *\n",
    "\n",
    "\n",
    "# settings\n",
    "model_type = \"openrouter\"\n",
    "ticker = 'BHP'\n",
    "ticker_profile = \"BHP Group Limited operates as a resources company in Australia, Europe, China, Japan, India, South Korea, the rest of Asia, North America, South America, and internationally. The company operates through Copper, Iron Ore, and Coal segments. It engages in the mining of copper, uranium, gold, zinc, lead, molybdenum, silver, iron ore, cobalt, and metallurgical and energy coal. The company is also involved in the mining, smelting, and refining of nickel, as well as potash development activities. In addition, it provides towing, freight, marketing and trading, marketing support, finance, administrative, and other services. The company was founded in 1851 and is headquartered in Melbourne, Australia.\"\n",
    "version = '1.0.0'\n",
    "test = True\n",
    "\n",
    "# Cut the input text to paragraph, if False it will cut to PDF pages\n",
    "cut_in_paragraph = False\n",
    "template_file_path = './prompts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813e5ff-e1de-410b-aa79-9eddec0b8080",
   "metadata": {},
   "source": [
    "# LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c517b5e8-a5ea-4ae4-ab1f-cc640a4502e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use MLX_LLM in the background\n",
    "\n",
    "if model_type == \"mlx\":\n",
    "    # local models\n",
    "    model_name = 'mlx-community/Meta-Llama-3.1-8B-Instruct-4bit'\n",
    "    # 8 bit cab fit in M4 memmory but seems the 4bit enough for our task\n",
    "    # so do not justfly the double memory usage\n",
    "    # model_name = 'mlx-community/Meta-Llama-3.1-8B-Instruct-8bit')\n",
    "    # model_name = 'mlx-community/Qwen3-8B-6bit'\n",
    "    # model_name = 'mlx-community/gemma-3-12b-it-4bit-DWQ')\n",
    "\n",
    "    api_key = \"nem_kell\"\n",
    "\n",
    "    base_url = \"http://localhost:8000/v1\"\n",
    "elif model_type == \"openrouter\":\n",
    "    # openrouter models\n",
    "    model_name = \"qwen/qwen3-30b-a3b:free\"\n",
    "\n",
    "    with open('openrouter_key.txt', 'r') as file:\n",
    "        api_key = file.read()\n",
    "\n",
    "    base_url = \"https://openrouter.ai/api/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed1bb47-7eb6-4947-806b-15a83586cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0, \n",
    "    model_name=model_name, \n",
    "    openai_api_base=base_url,\n",
    "    api_key=api_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a8b52-514b-4ae7-8081-eff425bdbbc7",
   "metadata": {},
   "source": [
    "# Read data\n",
    "\n",
    "Read PDF financial reports and process them to Langchain Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99221d3a-e750-4786-84bc-ffeb569e1b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: knowledge/bhp_20240701_20241231_qa_1.pdf file\n"
     ]
    }
   ],
   "source": [
    "paragraphs = get_paragraphs(ticker)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef8924e-9097-498a-95d2-0c55aff52463",
   "metadata": {},
   "source": [
    "# CrewAI knowedge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b67f1372-e0bb-408f-a733-2f4154eedb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai.knowledge.storage.knowledge_storage import KnowledgeStorage\n",
    "from crewai.utilities.paths import db_storage_path\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import pymilvus\n",
    "from pymilvus import model as pymilvus_model\n",
    "import hashlib\n",
    "import sys\n",
    "\n",
    "from typing import Any, Dict, List, Optional, Union, cast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e2b7c1b-b1ee-47cf-b453-e7cc61875f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MilvusKnowledgeStorage:\n",
    "    \"\"\"\n",
    "    Extends Storage to handle embeddings for memory entries, improving\n",
    "    search efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedder: Optional[Dict[str, Any]] = None,\n",
    "        collection_name: Optional[str] = \"knowledge\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the MilvusKnowledgeStorage with an optional embedder and collection name.\n",
    "\n",
    "        Args:\n",
    "            embedder (Optional[Dict[str, Any]]): An optional embedder for encoding documents\n",
    "                into embeddings. If not provided, a default embedder will be created.\n",
    "            collection_name (Optional[str]): The name of the collection to use in Milvus.\n",
    "                If not provided, it defaults to \"knowledge\".\n",
    "        \"\"\"\n",
    "\n",
    "        self.collection_name = collection_name\n",
    "        self.db_path = os.path.join(\"knowledge_milvus.db\")\n",
    "        self.app = pymilvus.MilvusClient(self.db_path)\n",
    "\n",
    "        # load collection if exist\n",
    "        if self.app.has_collection(collection_name=self.collection_name):\n",
    "            self.app.load_collection(collection_name=self.collection_name)\n",
    "\n",
    "        if embedder is not None:\n",
    "            self.embedder = embedder\n",
    "        else:\n",
    "            # use default embedder\n",
    "            # this will use HuggingFace FinLang embeddings\n",
    "            # see: https://huggingface.co/FinLang/finance-embeddings-investopedia\n",
    "            # for more details\n",
    "            self.embedder = self._create_default_embedding_function()\n",
    "\n",
    "    def search(\n",
    "        self,\n",
    "        query: List[str],\n",
    "        limit: int = 0,\n",
    "        filter: Optional[dict] = None, # not used at this moment\n",
    "        score_threshold: float = 0.35,\n",
    "    ) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Searches the Milvus collection for documents similar to the provided query.\n",
    "\n",
    "        Args:\n",
    "            query (List[str]): A list of query strings to search for in the collection.\n",
    "            limit (int): The maximum number of results to return. If 0, returns all results.\n",
    "            filter (Optional[dict]): A filter to apply to the search results. Not used\n",
    "                in this implementation, but can be extended in the future.\n",
    "            score_threshold (float): The minimum score threshold for results to be included.\n",
    "                Results with a score below this threshold will be excluded.\n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of dictionaries containing the search results,\n",
    "                where each dictionary contains the document ID, metadata, context, and score.\n",
    "        \"\"\"\n",
    "\n",
    "        fetched = self.app.search(\n",
    "            collection_name=self.collection_name,\n",
    "            data=self.embedder.encode_documents(query),\n",
    "            limit=30,\n",
    "            search_params={\"metric_type\": \"COSINE\", \"params\": {}},\n",
    "            output_fields=[\n",
    "                    \"text\", \n",
    "            ],\n",
    "        )\n",
    "\n",
    "        results = []\n",
    "\n",
    "        for one_query_result in fetched:  \n",
    "            for doc in one_query_result:\n",
    "                result = {\n",
    "                    'id': doc['id'],  \n",
    "                    'context': doc['entity']['text'],\n",
    "                    'score': doc['distance']\n",
    "                }\n",
    "                if result[\"score\"] >= score_threshold:\n",
    "                    results.append(result)\n",
    "                if limit > 0 and len(results) >= limit:\n",
    "                    break\n",
    "\n",
    "        return results\n",
    "\n",
    "    def initialize_knowledge_storage(self):\n",
    "        \"\"\"\n",
    "        Initializes the Milvus knowledge storage by creating a collection if it does not exist.\n",
    "        If the collection already exists, it resets the storage by dropping existing collections\n",
    "        and creating a new one.\n",
    "        \"\"\"\n",
    "\n",
    "        # clear previous dataset if exist\n",
    "        if self.app.has_collection(collection_name=self.collection_name):\n",
    "            self.reset()\n",
    "\n",
    "        if not self.app.has_collection(collection_name=self.collection_name):\n",
    "            print(f'Create {self.collection_name} collection')\n",
    "\n",
    "            # Create schema\n",
    "            schema = self.app.create_schema(\n",
    "                auto_id=True,\n",
    "                enable_dynamic_field=True,\n",
    "            )\n",
    "\n",
    "            dimension = 768 # dimension for HuggingFace FinLang\n",
    "\n",
    "            # add fields to schema\n",
    "            schema.add_field(field_name=\"id\", datatype=pymilvus.DataType.INT64, is_primary=True)\n",
    "            schema.add_field(field_name=\"vector\", datatype=pymilvus.DataType.FLOAT_VECTOR, dim=dimension)\n",
    "            schema.add_field(field_name=\"text\", datatype=pymilvus.DataType.VARCHAR, max_length=20000)\n",
    "\n",
    "            index_params = self.app.prepare_index_params()\n",
    "            index_params.add_index(\n",
    "                field_name=\"vector\",\n",
    "                index_type=\"AUTOINDEX\",\n",
    "                metric_type=\"COSINE\"\n",
    "            )\n",
    "\n",
    "            # Collection does not exist create it\n",
    "            self.app.create_collection(\n",
    "                collection_name=self.collection_name,\n",
    "                dimension=dimension,\n",
    "                schema=schema,\n",
    "                index_params=index_params,\n",
    "                consistency_level='Strong', # need for hybrid search,\n",
    "                vector_field=[\"vector\"],\n",
    "            )\n",
    "\n",
    "        if self.app:\n",
    "            self.app.load_collection(collection_name=self.collection_name)\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the Milvus knowledge storage by dropping all collections and creating a new one.\n",
    "        \"\"\"\n",
    "        if not self.app:\n",
    "            self.app = pymilvus.MilvusClient(self.db_path)\n",
    "\n",
    "        # delete collections\n",
    "        collections = self.app.list_collections()\n",
    "        for collection in collections:\n",
    "            self.app.drop_collection(collection_name=collection)\n",
    "\n",
    "        # create new app\n",
    "        self.app.close()\n",
    "        sleep(3)\n",
    "        self.app = pymilvus.MilvusClient(self.db_path)\n",
    "\n",
    "\n",
    "    def save(\n",
    "            self,\n",
    "            documents: List[str],\n",
    "            metadata: Optional[Union[Dict[str, Any], List[Dict[str, Any]]]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Saves documents and their metadata to the Milvus collection.\n",
    "\n",
    "        Args:\n",
    "            documents (List[str]): A list of document strings to be saved.\n",
    "            metadata (Optional[Union[Dict[str, Any], List[Dict[str, Any]]]]): Optional metadata associated with the documents.\n",
    "                If provided as a list, it should match the length of the documents list.\n",
    "                If provided as a single dictionary, it will be applied to all documents.\n",
    "\n",
    "        \"\"\"\n",
    "        if not self.app:\n",
    "            self.app = pymilvus.MilvusClient(self.db_path)\n",
    "\n",
    "        # Create a dictionary to store unique documents\n",
    "        unique_docs = {}\n",
    "\n",
    "        # Generate IDs and create a mapping of id -> (document, metadata)\n",
    "        for idx, doc in enumerate(documents):\n",
    "            doc_id = hashlib.sha256(doc.encode(\"utf-8\")).hexdigest()\n",
    "            doc_metadata = None\n",
    "            if metadata is not None:\n",
    "                if isinstance(metadata, list):\n",
    "                    doc_metadata = metadata[idx]\n",
    "                else:\n",
    "                    doc_metadata = metadata\n",
    "            unique_docs[doc_id] = (doc, doc_metadata)\n",
    "\n",
    "        # prepare data\n",
    "        data = []\n",
    "        data_size = 0\n",
    "        unique_docs = [ {'text': doc, 'metadata': meta} for doc, meta in unique_docs.values() ]\n",
    "\n",
    "        uniques_texts = [ doc['text'] for doc in unique_docs ]\n",
    "        uniques_docs_embeddings = self.embedder.encode_documents(uniques_texts)\n",
    "        for i in range(len(unique_docs)):\n",
    "            this_doc = {\n",
    "                'vector': uniques_docs_embeddings[i],\n",
    "                'text': unique_docs[i]['text'],\n",
    "            }            \n",
    "\n",
    "            data.append(this_doc)\n",
    "            data_size += sys.getsizeof(this_doc)\n",
    "\n",
    "        print(f'Loading data. Size: {data_size}')\n",
    "        try:\n",
    "            # batch load as Milvus has a 64Mb load limit, \n",
    "            # see: https://milvus.io/docs/limitations.md#Input-and-Output-per-RPC\n",
    "            avg = len(data)/ 2\n",
    "            last = 0\n",
    "            while last < len(data):\n",
    "                slice = data[int(last):int(last + avg)]\n",
    "                self.app.insert(collection_name=self.collection_name, data=slice)\n",
    "                last += avg\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to upsert documents: {e}\")\n",
    "            raise\n",
    "\n",
    "    def _create_default_embedding_function(self) -> pymilvus_model.dense.SentenceTransformerEmbeddingFunction:\n",
    "        \"\"\"\n",
    "        Creates a default embedding function using the HuggingFace FinLang embeddings.\n",
    "        This function uses the 'FinLang/finance-embeddings-investopedia' model to encode\n",
    "        documents into embeddings.\n",
    "\n",
    "        Returns:\n",
    "            pymilvus_model.dense.SentenceTransformerEmbeddingFunction: An instance of the embedding function.\n",
    "        \"\"\"\n",
    "        return pymilvus_model.dense.SentenceTransformerEmbeddingFunction(\n",
    "            model_name='FinLang/finance-embeddings-investopedia'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f626ded-14b0-4b84-97cd-f1076990a559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create knowledge collection\n",
      "Loading data. Size: 8464\n",
      "[\n",
      "    {\n",
      "        \"id\": 460511664591077443,\n",
      "        \"context\": \"#  **EPHREM RAVI, CITIGROUP**\\n\\nMy first question is on the smelter and refinery expansion at Olympic Dam, the first phase of FID by 2027. What could be the order of magnitude of capex for that? Is it very simplistically to reduce the uranium levels and take more ore from the OZ Minerals assets? When would that expansion be ready, assuming it is approved? Are we talking by 2030, or beyond that?\",\n",
      "        \"score\": 0.7024557590484619\n",
      "    },\n",
      "    {\n",
      "        \"id\": 460511664591077444,\n",
      "        \"context\": \"#  **MIKE HENRY, BHP**\\n\\nOkay, so in order, on the capex we have not yet provided information on capital intensity. We will in due course, as we have done with the Chilean projects, but we want to do it once we have got a next level of confidence around it. I would say that, at a macro level, the economics of that project are going to be supported by the synergies to be unlocked, which you pointed to in the second part of your question around the OZ Minerals ores.  \\nThere is a choice for us to make in bringing the OZ Minerals ores through the newly upgraded and expanded smelter, and that is as to whether we were to expand uranium processing capacity in parallel with that or not. The current indication is that the economics of that may be challenged. However, there is still a significant uranium benefit to us in that the Prominent Hill and Carrapateena ores currently incur a uranium penalty that we would remove. The synergies here are removal of the uranium penalty and the reduction in transport distance. They are two big drivers of that, and that will support the economics of the SRE project.  \\nThe final part of your question was around timing. You are right; we are looking at FID in circa 2027. The capacity would then be online right at the start of the 2030s and just beyond that.  \\nI just want to thank everybody for joining this evening. I know it is early morning for many of you. I hope that, coming through in the results, you can see another very reliable set of operational results, strong financial results, and increasing things to be positive about in terms of the growth optionality in the portfolio, both in potash and in copper, which is going to underpin attractive shareholder returns and value growth over the decades to come. Thank you.\",\n",
      "        \"score\": 0.570316731929779\n",
      "    },\n",
      "    {\n",
      "        \"id\": 460511664591077438,\n",
      "        \"context\": \"#  **VANDITA PANT, BHP**\\n\\nOn FX, the Australian dollar has not really materially moved to make an impact on the capex side, but CLP has. The Chilean peso would be a benefit, and we are seeing that come through in some of the projects.\",\n",
      "        \"score\": 0.4192606508731842\n",
      "    },\n",
      "    {\n",
      "        \"id\": 460511664591077437,\n",
      "        \"context\": \"#  **MIKE HENRY, BHP**\\n\\nVandita, maybe you can answer the question on FX impact on capex. I just want to start, though, Matt, with your comment; you referred to the US\\\\$11 billion as a ceiling. I want to be clear that that is our average guidance on capex. It is not a target and it is not a ceiling. We have noted that we have the ability to flex upwards or downwards on that US\\\\$11 billion, depending on the opportunity set available to us and depending upon cash inflows into the company. Your point on Resolution, Vicu\\u00f1a and Antamina is taken, and those would be equity-accounted. I do note that the US\\\\$11 billion does not back out the joint venture partner shares out of something like Escondida. We have to look at it from both sides of the equation.\",\n",
      "        \"score\": 0.4156718850135803\n",
      "    },\n",
      "    {\n",
      "        \"id\": 460511664589242370,\n",
      "        \"context\": \"#  **LIAM FITZPATRICK, DEUTSCHE BANK**\\n\\nGood morning, or good evening, Mike and team, I have two questions. First, on Vicu\\u00f1a, can you give us a bit of an update on the timing in terms of when you think you will be in a position to announce capex for the project and potentially when you could make an investment decision on the first phase?\",\n",
      "        \"score\": 0.3662809729576111\n",
      "    },\n",
      "    {\n",
      "        \"id\": 460511664589242386,\n",
      "        \"context\": \"#  **ALEXANDER PEARCE, BMO**\\n\\nMy question is around Jansen. Westshore flagged a couple of times last year that the upgrades it needs to make to the port were presenting some challenges from a cost perspective. I wondered if there are any concerns from you that there could be some time slippage associated with that project, or indeed any cost implications from BHP?\",\n",
      "        \"score\": 0.3658982813358307\n",
      "    },\n",
      "    {\n",
      "        \"id\": 460511664589242374,\n",
      "        \"context\": \"#  **JASON FAIRCLOUGH, BANK OF AMERICA**\\n\\nMike, thanks for making time for us at the end of the day. I know that it is a busy day, so I really appreciate these calls, as usual. I have a couple of questions. The first is on your medium term capex guide. It looks like you are guiding for US\\\\$10 billion to US\\\\$11 billion per year. Does that include the capex for the Escondida recapitalisation? So the US\\\\$10.5 billion to US\\\\$14.4 billion to stand still, is that included in that US\\\\$10 billion to US\\\\$11 billion or is it on top of that?\",\n",
      "        \"score\": 0.3579156994819641\n",
      "    },\n",
      "    {\n",
      "        \"id\": 460511664591077436,\n",
      "        \"context\": \"#  **MATT GREENE, GOLDMAN SACHS**\\n\\nI have a couple of questions, one on capex and one on Jansen. On capex, it is quite encouraging to see some of the FX tailwinds in unit costs, but can you just comment on how the weaker FX is translating into your capex? Just on the US\\\\$11 billion ceiling, I guess we are starting to see your equity-accounted investments potentially going up around Resolution, Vicu\\u00f1a, some infrastructure around Jansen, and potentially Antamina in the future. How should we be thinking about investing cash outflow of the company on a medium-term outlook?\",\n",
      "        \"score\": 0.3530730903148651\n",
      "    },\n",
      "    {\n",
      "        \"id\": 460511664589242378,\n",
      "        \"context\": \"#  **AMOS FLETCHER, BARCLAYS**\\n\\nGood morning, Mike. Thanks for the opportunity. What is your current thinking on the WAIO 330 expansion and, in particular, whether you see the market needing an extra 25 million tonnes of iron ore, given the progress being made in Guinea, and China demand, I am assuming, having peaked in 2020?\",\n",
      "        \"score\": 0.35166051983833313\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "if test:\n",
    "    # Initialize Milvus DB\n",
    "    knowledge = MilvusKnowledgeStorage()\n",
    "    knowledge.initialize_knowledge_storage()\n",
    "\n",
    "    # load data\n",
    "    documents = [ doc.page_content for doc in paragraphs]\n",
    "    metadata = [ doc.metadata for doc in paragraphs]\n",
    "    knowledge.save(documents=documents, metadata=metadata)\n",
    "\n",
    "    # search in the DB\n",
    "    print(json.dumps(knowledge.search(\n",
    "        [\"My first question is on the smelter and refinery expansion at Olympic Dam, the first phase of FID by 2027. What could be the order of magnitude of capex for that? Is it very simplistically to reduce the uranium levels and take more ore from the OZ Minerals assets? When would that expansion be ready, assuming it is approved? Are we talking by 2030, or beyond that\"],\n",
    "        limit=10,\n",
    "        score_threshold=0.35\n",
    "    ), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cef0e2-ed6f-4bdc-b9d8-7138cab801df",
   "metadata": {},
   "source": [
    "# Implement RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e6e7209-579d-41b4-bcf9-25aa1a15a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_template(filename: str) -> Callable:\n",
    "    \"\"\"\n",
    "    Load a prompt template from a YAML file.\n",
    "\n",
    "    Args:\n",
    "        filename (str): The name of the YAML file containing the prompt template.\n",
    "    Returns:\n",
    "        Callable: A callable function that takes a dictionary of parameters and returns a formatted prompt.\n",
    "    Raises:\n",
    "        ValueError: If the specified file does not exist or is not a valid file.\n",
    "    \"\"\"\n",
    "    # open file\n",
    "    prompt_file = os.path.join(template_file_path, filename)\n",
    "    # test file exist\n",
    "    if not os.path.isfile(prompt_file):\n",
    "        raise ValueError(f\"{prompt_file} not a valid file\")\n",
    "    with open(prompt_file) as file:\n",
    "        source = yaml.safe_load(file)\n",
    "\n",
    "    compiler = Compiler()\n",
    "    # Compile the system template\n",
    "    prompt_template = compiler.compile(source['prompt_template'])\n",
    "\n",
    "    return prompt_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e9e97b6-2dbb-4a45-84b7-14e9f67433f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRAG:\n",
    "    \"\"\"\n",
    "    A simple RAG implementation that uses Milvus as a knowledge base.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: List[Document]=None):\n",
    "        \"\"\"\n",
    "        Initializes the MyRAG instance with the provided data.\n",
    "        Args:\n",
    "            data (List[Document]): A list of Langchain Document objects containing the knowledge base.\n",
    "\n",
    "        \"\"\"\n",
    "        # Initialize Milvus DB\n",
    "        self.knowledge = MilvusKnowledgeStorage()\n",
    "\n",
    "        if data is not None:\n",
    "            self.knowledge.initialize_knowledge_storage()\n",
    "            # load data\n",
    "            documents = [ doc.page_content for doc in data]\n",
    "            metadata = [ doc.metadata for doc in data]\n",
    "            self.knowledge.save(documents=documents, metadata=metadata)\n",
    "\n",
    "        # system prompt\n",
    "        system_prompt_template = load_template(\"basic_rag_system.yaml\")\n",
    "        self.system_prompt = system_prompt_template({})\n",
    "\n",
    "    def invoke(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Invokes the RAG system to answer a question using the knowledge base.\n",
    "\n",
    "        Args:\n",
    "            question (str): The question to be answered.\n",
    "        Returns:\n",
    "            str: The answer to the question generated by the RAG system.\n",
    "        Raises:\n",
    "            ValueError: If the question is empty or not provided.\n",
    "        \"\"\"\n",
    "        # get result from knowledge \n",
    "        context = json.dumps(\n",
    "            self.knowledge.search(\n",
    "                [question],\n",
    "                limit=10,\n",
    "                score_threshold=0.15\n",
    "            ),\n",
    "            indent=4\n",
    "        )\n",
    "        user_prompt_template = load_template(\"basic_rag_user.yaml\")\n",
    "        user_prompt = user_prompt_template({\n",
    "            \"context\": context,\n",
    "            \"question\": question\n",
    "        })\n",
    "\n",
    "        messages = [\n",
    "            ('system', self.system_prompt),\n",
    "            ('user', user_prompt)\n",
    "        ]\n",
    "\n",
    "        with tracer.start_as_current_span(\"basic_rag\") as child_span:\n",
    "            child_span.set_attribute(SpanAttributes.INPUT_VALUE, context)\n",
    "            child_span.set_attribute(SpanAttributes.LLM_MODEL_NAME, model_name)\n",
    "\n",
    "            try:\n",
    "                rag_answer = llm.invoke(messages).content\n",
    "            except Exception as e:\n",
    "                child_span.set_attribute(SpanAttributes.OUTPUT_VALUE, str(e))\n",
    "                child_span.set_status(Status(StatusCode.ERROR))\n",
    "                return f\"Error in LLM invocation: {f}\"\n",
    "\n",
    "            \n",
    "            child_span.set_attribute(SpanAttributes.OUTPUT_VALUE, rag_answer)\n",
    "            child_span.set_status(Status(StatusCode.OK))\n",
    "            return rag_answer\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1a10d0f-4b5f-4402-ad54-3d917518f33d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create knowledge collection\n",
      "Loading data. Size: 8464\n",
      "The smelter and refinery expansion at Olympic Dam, with the first phase of Final Investment Decision (FID) targeted for **circa 2027**, is expected to have **capacity online by the start of the 2030s** (i.e., around **2030 or slightly beyond**), according to Mike Henry from BHP. However, the **exact order of magnitude for the capex** (capital expenditure) is not explicitly provided in the context. \n",
      "\n",
      "The expansion is linked to **synergies from OZ Minerals' ores**, such as **removing a uranium penalty** and **reducing transport distances**, which would improve economics. While the question simplistically frames the expansion as \"reducing uranium levels and taking more ore,\" the context emphasizes that the project's viability depends on these synergies rather than a straightforward approach. \n",
      "\n",
      "No specific capex figure is mentioned, but the timing aligns with the 2030s. For precise financial details, further updates from BHP would be required.\n"
     ]
    }
   ],
   "source": [
    "my_rag = MyRAG(paragraphs)\n",
    "question = \"My first question is on the smelter and refinery expansion at Olympic Dam, the first phase of FID by 2027. What could be the order of magnitude of capex for that? Is it very simplistically to reduce the uranium levels and take more ore from the OZ Minerals assets? When would that expansion be ready, assuming it is approved? Are we talking by 2030, or beyond that\"\n",
    "print(my_rag.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86e5f6be-db75-4c7a-b1cd-ad4afdd76ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The smelter and refinery expansion at Olympic Dam, with the first phase of Final Investment Decision (FID) targeted for 2027, is expected to be **online by the start of the 2030s** (i.e., around 2030 or slightly beyond). However, the **exact order of magnitude for the capex** (capital expenditure) is not explicitly stated in the provided context. \n",
      "\n",
      "Mike Henry from BHP notes that the project's economics will be supported by **synergies from processing OZ Minerals ores**, including **removing uranium penalties** and **reducing transport distances**. While the expansion is likely tied to optimizing uranium processing and increasing ore throughput from OZ Minerals assets, specific capex figures are not detailed here. \n",
      "\n",
      "For precise capex estimates, further updates from BHP or additional context would be required. The timeline aligns with the 2030s, but no definitive confirmation beyond that is provided.\n"
     ]
    }
   ],
   "source": [
    "my_rag_2 = MyRAG()\n",
    "print(my_rag_2.invoke(question))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "518fffe3-aa7e-4677-9ae8-15ebe56f5a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  **MIKE HENRY, BHP**\n",
      "\n",
      "Okay, so in order, on the capex we have not yet provided information on capital intensity. We will in due course, as we have done with the Chilean projects, but we want to do it once we have got a next level of confidence around it. I would say that, at a macro level, the economics of that project are going to be supported by the synergies to be unlocked, which you pointed to in the second part of your question around the OZ Minerals ores.  \n",
      "There is a choice for us to make in bringing the OZ Minerals ores through the newly upgraded and expanded smelter, and that is as to whether we were to expand uranium processing capacity in parallel with that or not. The current indication is that the economics of that may be challenged. However, there is still a significant uranium benefit to us in that the Prominent Hill and Carrapateena ores currently incur a uranium penalty that we would remove. The synergies here are removal of the uranium penalty and the reduction in transport distance. They are two big drivers of that, and that will support the economics of the SRE project.  \n",
      "The final part of your question was around timing. You are right; we are looking at FID in circa 2027. The capacity would then be online right at the start of the 2030s and just beyond that.  \n",
      "I just want to thank everybody for joining this evening. I know it is early morning for many of you. I hope that, coming through in the results, you can see another very reliable set of operational results, strong financial results, and increasing things to be positive about in terms of the growth optionality in the portfolio, both in potash and in copper, which is going to underpin attractive shareholder returns and value growth over the decades to come. Thank you.\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs[-1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "267aa940-779c-4dd5-bbfa-653f2c1c522e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#  **EPHREM RAVI, CITIGROUP**\n",
      "\n",
      "My first question is on the smelter and refinery expansion at Olympic Dam, the first phase of FID by 2027. What could be the order of magnitude of capex for that? Is it very simplistically to reduce the uranium levels and take more ore from the OZ Minerals assets? When would that expansion be ready, assuming it is approved? Are we talking by 2030, or beyond that?\n"
     ]
    }
   ],
   "source": [
    "print(paragraphs[-2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203bd86-514e-4dad-af34-b8e4c74b39f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
