{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c145b6f-bcc0-44a5-94f0-e9ee174b0243",
   "metadata": {},
   "source": [
    "# Set up Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ab7110d-97e2-40e6-b4c8-f7c36b7b3c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/ds_python311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "import os\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "\n",
    "project_name = \"Basic_RAG\"\n",
    "\n",
    "# Add Phoenix API Key for tracing\n",
    "phoenix_key = ''\n",
    "with open('phoenix_key.txt', 'r') as file:\n",
    "    phoenix_key = file.read()\n",
    "os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={phoenix_key}\"\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={phoenix_key}\";\n",
    "os.environ['PHOENIX_PROJECT_NAME'] = project_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dcab46-a2e4-4d9c-a4c6-6589fdd815e6",
   "metadata": {},
   "source": [
    "# Basic imports and setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f124b273-6396-41f0-8bc3-1ad670260f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b4160c-4d18-4d25-a45e-4251493ece87",
   "metadata": {},
   "source": [
    "## Load best dataset from Phoenix\n",
    "\n",
    "Generated the answer with Anthropic Claude 3 Opus model. No noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13d67d6b-a5cf-44a4-ac98-e66758389f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoenix_client = px.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e89f0c64-311a-4566-aceb-a0c918a7290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_best = pd.read_json(\"./best_qa_data_bhp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62e8a40f-386f-4e29-802f-bdc210afadd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì§ Uploading dataset...\n"
     ]
    }
   ],
   "source": [
    "dataset_phoenix_name = \"best_qa_data_bhp\"\n",
    "try:\n",
    "    dataset_best = phoenix_client.upload_dataset(\n",
    "        dataframe=df_best,\n",
    "        dataset_name=dataset_phoenix_name,\n",
    "        input_keys=[\"question\"],\n",
    "        output_keys=[\"human\",  \"chatgpt\"],\n",
    "    )\n",
    "except Exception as e:\n",
    "    dataset_best = phoenix_client.get_dataset(name=dataset_phoenix_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dee31d-cc83-4c72-99d6-7a990ff7019d",
   "metadata": {},
   "source": [
    "# Write Phoenix evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98fa7f3d-07ad-4a01-a131-948e21f43997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.evaluators.base import EvaluationResult, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "343ec3f9-dbea-4edc-986b-1893d626b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import torch\n",
    "from bert_score import score\n",
    "\n",
    "class BERTScore(Evaluator):\n",
    "    name=\"BERT Score\"\n",
    "    def evaluate(self, output: str, expected: Dict[str, Any], **kwargs) -> EvaluationResult:\n",
    "        expected_answer = expected[\"human\"]\n",
    "\n",
    "        # compute Bert score\n",
    "        # presission, recall and F1\n",
    "        P, R, F1 = score([output], [expected_answer], lang=\"en\", model_type=\"ProsusAI/finbert\")\n",
    "        return EvaluationResult(score=F1.numpy(force=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3982ab73-d675-41e7-8708-9a395921fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from scipy.spatial import distance\n",
    "\n",
    "class USESimilarity(Evaluator):\n",
    "    name=\"USE\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\")\n",
    "    def evaluate(self, output: str, expected, **kwargs) -> EvaluationResult:\n",
    "        embeddings = self.embed([\n",
    "            output,\n",
    "            expected[\"human\"]\n",
    "        ])\n",
    "\n",
    "        similarity = 1 - distance.cosine(embeddings[0], embeddings[1])\n",
    "        return EvaluationResult(score=similarity)\n",
    "\n",
    "uses = USESimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70eaf18b-abfd-4761-af33-a5c79d72a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "import nltk\n",
    "\"\"\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "\n",
    "def my_tokkenizer(text):\n",
    "    # k√ºl√∂nleges karakterek\n",
    "    pattern = r\"[{}]\".format(\"(),.;:%\\\"\") \n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    \n",
    "    # kisbet≈±\n",
    "    text = text.lower()\n",
    "    # felesleges √ºres mez≈ëk t√∂rl√©se \n",
    "    text = text.strip()\n",
    " \n",
    "    # szavakra v√°g√°s\n",
    "    from nltk.tokenize import WordPunctTokenizer\n",
    "    WPT = WordPunctTokenizer()\n",
    "    tokens = WPT.tokenize(text)\n",
    "\n",
    "    # stop szavak elt√°vol√≠t√°sa\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens \n",
    "        if token not in stop_word_list\n",
    "    ]\n",
    "    \n",
    "    # Lemmatize do not need as METEOR handle this also\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "class Meteor(Evaluator):\n",
    "    name=\"METEOR\"\n",
    "    def evaluate(self, output: str, expected, **kwargs) -> EvaluationResult:\n",
    "        score = single_meteor_score(my_tokkenizer(output), my_tokkenizer(expected[\"human\"]))\n",
    "        return EvaluationResult(score=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "180d75ec-6dda-4f92-81a7-b2cca72bf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "\n",
    "class SBERTFinance(Evaluator):\n",
    "    name=\"SBERT finance\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SentenceTransformer(\n",
    "            \"FinLang/finance-embeddings-investopedia\", \n",
    "            similarity_fn_name=SimilarityFunction.COSINE\n",
    "        )\n",
    "    def evaluate(self, output: str, expected, **kwargs) -> EvaluationResult:\n",
    "        rag_answer_embeddings = self.model.encode([output])\n",
    "        expected_embeddings = self.model.encode([expected[\"human\"]])\n",
    "        similarity = self. model.similarity(rag_answer_embeddings, expected_embeddings)\n",
    "        return EvaluationResult(score=similarity.numpy(force=True)[0][0])\n",
    "\n",
    "finance = SBERTFinance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3e750282-cb7e-4155-8811-06b5d03b112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERTQwen4(Evaluator):\n",
    "    name=\"SBERT Qwen3 4B\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SentenceTransformer(\n",
    "            \"Qwen/Qwen3-Embedding-4B\", \n",
    "            similarity_fn_name=SimilarityFunction.COSINE,\n",
    "            tokenizer_kwargs={\"padding_side\": \"left\"},\n",
    "        )\n",
    "    def evaluate(self, output: str, expected, **kwargs) -> EvaluationResult:\n",
    "        rag_answer_embeddings = self.model.encode([output])\n",
    "        expected_embeddings = self.model.encode([expected[\"human\"]])\n",
    "        similarity = self.model.similarity(rag_answer_embeddings, expected_embeddings)\n",
    "        return EvaluationResult(score=similarity.numpy(force=True)[0][0])\n",
    "\n",
    "class SBERTQwen06(Evaluator):\n",
    "    name=\"SBERT Qwen3 0.6B\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SentenceTransformer(\n",
    "            \"Qwen/Qwen3-Embedding-0.6B\", \n",
    "            similarity_fn_name=SimilarityFunction.COSINE,\n",
    "            model_kwargs={\"device_map\": \"auto\"},\n",
    "            tokenizer_kwargs={\"padding_side\": \"left\"},\n",
    "        )\n",
    "    def evaluate(self, output: str, expected, **kwargs) -> EvaluationResult:\n",
    "        rag_answer_embeddings = self.model.encode([output])\n",
    "        expected_embeddings = self.model.encode([expected[\"human\"]])\n",
    "        similarity = self.model.similarity(rag_answer_embeddings, expected_embeddings)\n",
    "        return EvaluationResult(score=similarity.numpy(force=True)[0][0])\n",
    "\n",
    "# qwen4 = SBERTQwen4()\n",
    "qwen06 = SBERTQwen06()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895baf31-bcc8-440d-897f-5d17dc6afe07",
   "metadata": {},
   "source": [
    "# Knowledge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ec2a55d-fb03-4e0d-8c51-089099efe0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.instrumentation.instrumentor:Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: Basic_RAG\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'api_key': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:opentelemetry.trace:Overriding of current TracerProvider is not allowed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded layout model s3://layout/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded texify model s3://texify/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded recognition model s3://text_recognition/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded table recognition model s3://table_recognition/2025_02_18 on device mps with dtype torch.float16\n",
      "Loaded detection model s3://text_detection/2025_02_28 on device mps with dtype torch.float16\n",
      "Loaded detection model s3://inline_math_detection/2025_02_24 on device mps with dtype torch.float16\n",
      "Processing: knowledge/rio_20240701_20240930_qa_1.pdf file\n",
      "Processing: knowledge/rio_20241001_20241231_qa_1.pdf file\n",
      "305\n",
      "Create knowledge collection\n",
      "Loading data. Size: 56120\n"
     ]
    }
   ],
   "source": [
    "from basic_rag import MilvusKnowledgeStorage, get_paragraphs\n",
    "\n",
    "paragraphs = get_paragraphs(\"RIO\")\n",
    "# paragraphs += get_paragraphs(\"RIO\")\n",
    "print(len(paragraphs))\n",
    "\n",
    "knowledge = MilvusKnowledgeStorage()\n",
    "knowledge.initialize_knowledge_storage()\n",
    "\n",
    "# load data\n",
    "documents = [ doc.page_content for doc in paragraphs]\n",
    "metadata = [ doc.metadata for doc in paragraphs]\n",
    "knowledge.save(documents=documents, metadata=metadata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e3eb27b-d1de-4726-b274-174d90cf3908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'filename': 'knowledge/rio_20240701_20240930_qa_1.pdf', 'reporting_year_start': 2024, 'reporting_quarter_start': 'Q3', 'reporting_year_end': 2024, 'reporting_quarter_end': 'Q3', 'document_type': 'qa', 'source': 'RIO', 'Header_2': '**Q&A**'}, page_content=\"#  **Q&A**\\n\\n**Ulric Adom:** The bulk of our efforts obviously are on the numerator. So that's really driving the EBITDA margin uplift that we have said. And that's really what will drive most of the effort.  \")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraphs[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdbda00-55da-4ee7-a912-2bb1f5a12b68",
   "metadata": {},
   "source": [
    "# Evaluate basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14567881-dcce-42a6-b627-b99f2531a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.types import Example\n",
    "from basic_rag import MyRAG\n",
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "def task(input, expected) -> str:\n",
    "    question = input['question']\n",
    "    \n",
    "    \n",
    "    # mock the RAG generation\n",
    "    rag_answer = MyRAG().invoke(question)\n",
    "    \n",
    "    return rag_answer\n",
    "\n",
    "experiment = run_experiment(\n",
    "    dataset_best,\n",
    "    task,\n",
    "    experiment_name=\"rag-experiment\",\n",
    "    evaluators=[qwen06, BERTScore() ],\n",
    ")\n",
    "\n",
    "evaluation_result = experiment.get_evaluations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f131e9e-bb8f-468e-bc06-8cbfe02d527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_template = {\n",
    "    \"layout\": plotly_go.Layout(\n",
    "        font={\n",
    "            \"family\": \"Nunito\",\n",
    "            \"size\": 12,\n",
    "            \"color\": \"#707070\",\n",
    "        },\n",
    "        title={\n",
    "            \"font\": {\n",
    "                \"family\": \"Lato\",\n",
    "                \"size\": 18,\n",
    "                \"color\": \"#1f1f1f\",\n",
    "            },\n",
    "        },\n",
    "        plot_bgcolor=\"#ffffff\",\n",
    "        paper_bgcolor=\"#ffffff\",\n",
    "        colorway=plotly_express.colors.qualitative.G10,\n",
    "    )\n",
    "}\n",
    "\n",
    "def format_title(title, subtitle=None, subtitle_font_size=14):\n",
    "    title = f'<b>{title}</b>'\n",
    "    if not subtitle:\n",
    "        return title\n",
    "    subtitle = f'<span style=\"font-size: {subtitle_font_size}px;\">{subtitle}</span>'\n",
    "    return f'{title}<br>{subtitle}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5327d5d-79dd-4003-bcc7-fa2ac3d4fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as plotly_express\n",
    "import plotly.graph_objects as plotly_go\n",
    "\n",
    "evaluation_result_filtered = pd.DataFrame({\n",
    "    \"name\": evaluation_result[\"name\"].values,\n",
    "    \"score\": evaluation_result[\"score\"].values\n",
    "})\n",
    "\n",
    "# Create a boxplot using Plotly\n",
    "fig = plotly_express.box(\n",
    "    evaluation_result_filtered,\n",
    "    x=\"name\",\n",
    "    y=\"score\",\n",
    "    title=format_title(\"Sz√∂veg-hasonl√≥s√°g m√©r√©se\", \"RAG megval√≥s√≠t√°s teljes√≠tm√©nye\"),\n",
    "    labels={\"name\": \"Evaluation Type\", \"score\": \"Similarity Score\"},\n",
    "    template=custom_template\n",
    ")\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"M√©r√©si m√≥dszer\",\n",
    "    yaxis_title=\"Hasonl√≥s√°g\",\n",
    "    xaxis=dict(tickangle=45),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c5e05-e405-4caf-bc2f-e1e8a475b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result_filtered.to_json(\"rag_validation_result_3.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05daf09f-9b02-48de-b8fc-e9b3212cc58e",
   "metadata": {},
   "source": [
    "# Cleaning up\n",
    "\n",
    "To cleanup the unnecessary HuggingFace models run the following command: huggingface-cli delete-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473ef4e-da39-40e7-b2e3-1660abedd476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816949c6-93d1-4050-b062-45b93376a4a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
