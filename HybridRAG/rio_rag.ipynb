{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c145b6f-bcc0-44a5-94f0-e9ee174b0243",
   "metadata": {},
   "source": [
    "# Set up Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d185e22e-bbfd-40c4-b452-c562817e3d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.otel import register\n",
    "import os\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "\n",
    "project_name = \"RAG_Rio\"\n",
    "\n",
    "# Add Phoenix API Key for tracing\n",
    "phoenix_key = ''\n",
    "with open('phoenix_key.txt', 'r') as file:\n",
    "    phoenix_key = file.read()\n",
    "os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={phoenix_key}\"\n",
    "os.environ[\"PHOENIX_API_KEY\"] = phoenix_key\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={phoenix_key}\";\n",
    "os.environ['PHOENIX_PROJECT_NAME'] = project_name\n",
    "\n",
    "# configure the Phoenix tracer\n",
    "tracer_provider = register(\n",
    "  project_name=project_name, # Default is 'default'\n",
    "  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n",
    ")\n",
    "OpenAIInstrumentor().instrument(tracer_provider = tracer_provider)\n",
    "tracer = tracer_provider.get_tracer(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6dcab46-a2e4-4d9c-a4c6-6589fdd815e6",
   "metadata": {},
   "source": [
    "# Basic imports and setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124b273-6396-41f0-8bc3-1ad670260f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import phoenix as px\n",
    "from basic_rag import MilvusKnowledgeStorage, get_paragraphs, search_pdf\n",
    "from time import sleep\n",
    "import datetime\n",
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b4160c-4d18-4d25-a45e-4251493ece87",
   "metadata": {},
   "source": [
    "## Load best dataset from Phoenix\n",
    "\n",
    "Generated the answer with Anthropic Claude 3 Opus model. No noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d67d6b-a5cf-44a4-ac98-e66758389f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "phoenix_client = px.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89f0c64-311a-4566-aceb-a0c918a7290b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_phoenix_name = \"qa_data_rio\"\n",
    "df_best = pd.read_json(f\"./{dataset_phoenix_name}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e8a40f-386f-4e29-802f-bdc210afadd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    dataset_best = phoenix_client.upload_dataset(\n",
    "        dataframe=df_best,\n",
    "        dataset_name=dataset_phoenix_name,\n",
    "        input_keys=[\"question\"],\n",
    "        output_keys=[\"human\",  \"chatgpt\"],\n",
    "    )\n",
    "except Exception as e:\n",
    "    dataset_best = phoenix_client.get_dataset(name=dataset_phoenix_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4dee31d-cc83-4c72-99d6-7a990ff7019d",
   "metadata": {},
   "source": [
    "# Write Phoenix evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fa7f3d-07ad-4a01-a131-948e21f43997",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.evaluators.base import EvaluationResult, Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343ec3f9-dbea-4edc-986b-1893d626b7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict\n",
    "import torch\n",
    "from bert_score import score\n",
    "\n",
    "class BERTScore(Evaluator):\n",
    "    name=\"BERT Score\"\n",
    "    def evaluate(self, output: str, expected: Dict[str, Any], **kwargs) -> EvaluationResult:\n",
    "        expected_answer = expected[\"chatgpt\"]\n",
    "\n",
    "        # compute Bert score\n",
    "        # presission, recall and F1\n",
    "        P, R, F1 = score([output], [expected_answer], lang=\"en\", model_type=\"ProsusAI/finbert\")\n",
    "        return EvaluationResult(score=F1.numpy(force=True)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3982ab73-d675-41e7-8708-9a395921fba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from scipy.spatial import distance\n",
    "\n",
    "class USESimilarity(Evaluator):\n",
    "    name=\"USE\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embed = hub.load(\"https://www.kaggle.com/models/google/universal-sentence-encoder/TensorFlow2/universal-sentence-encoder/2\")\n",
    "    def evaluate(self, output: str, expected, **kwargs) -> EvaluationResult:\n",
    "        embeddings = self.embed([\n",
    "            output,\n",
    "            expected[\"chatgpt\"]\n",
    "        ])\n",
    "\n",
    "        similarity = 1 - distance.cosine(embeddings[0], embeddings[1])\n",
    "        return EvaluationResult(score=similarity)\n",
    "\n",
    "uses = USESimilarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eaf18b-abfd-4761-af33-a5c79d72a765",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.meteor_score import single_meteor_score\n",
    "import nltk\n",
    "\"\"\"\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_word_list = stopwords.words('english')\n",
    "\n",
    "\n",
    "def my_tokkenizer(text):\n",
    "    # különleges karakterek\n",
    "    pattern = r\"[{}]\".format(\"(),.;:%\\\"\") \n",
    "    text = re.sub(pattern, \"\", text)\n",
    "    \n",
    "    # kisbetű\n",
    "    text = text.lower()\n",
    "    # felesleges üres mezők törlése \n",
    "    text = text.strip()\n",
    " \n",
    "    # szavakra vágás\n",
    "    from nltk.tokenize import WordPunctTokenizer\n",
    "    WPT = WordPunctTokenizer()\n",
    "    tokens = WPT.tokenize(text)\n",
    "\n",
    "    # stop szavak eltávolítása\n",
    "    filtered_tokens = [\n",
    "        token for token in tokens \n",
    "        if token not in stop_word_list\n",
    "    ]\n",
    "    \n",
    "    # Lemmatize do not need as METEOR handle this also\n",
    "\n",
    "    return filtered_tokens\n",
    "\n",
    "class Meteor(Evaluator):\n",
    "    name=\"METEOR\"\n",
    "    def evaluate(self, output: str, expected, **kwargs) -> EvaluationResult:\n",
    "        score = single_meteor_score(my_tokkenizer(output), my_tokkenizer(expected[\"chatgpt\"]))\n",
    "        return EvaluationResult(score=score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d75ec-6dda-4f92-81a7-b2cca72bf55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import euclidean\n",
    "from sentence_transformers import SentenceTransformer, SimilarityFunction\n",
    "\n",
    "class SBERTFinance(Evaluator):\n",
    "    name=\"SBERT finance\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SentenceTransformer(\n",
    "            \"FinLang/finance-embeddings-investopedia\", \n",
    "            similarity_fn_name=SimilarityFunction.COSINE\n",
    "        )\n",
    "    def evaluate(self, output: str, expected, **kwargs) -> EvaluationResult:\n",
    "        rag_answer_embeddings = self.model.encode([output])\n",
    "        expected_embeddings = self.model.encode([expected[\"chatgpt\"]])\n",
    "        similarity = self. model.similarity(rag_answer_embeddings, expected_embeddings)\n",
    "        return EvaluationResult(score=similarity.numpy(force=True)[0][0])\n",
    "\n",
    "finance = SBERTFinance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e750282-cb7e-4155-8811-06b5d03b112e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERTQwen4(Evaluator):\n",
    "    name=\"SBERT Qwen3 4B\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SentenceTransformer(\n",
    "            \"Qwen/Qwen3-Embedding-4B\", \n",
    "            similarity_fn_name=SimilarityFunction.COSINE,\n",
    "            tokenizer_kwargs={\"padding_side\": \"left\"},\n",
    "        )\n",
    "    def evaluate(self, output: str, expected, **kwargs) -> EvaluationResult:\n",
    "        rag_answer_embeddings = self.model.encode([output])\n",
    "        expected_embeddings = self.model.encode([expected[\"chatgpt\"]])\n",
    "        similarity = self.model.similarity(rag_answer_embeddings, expected_embeddings)\n",
    "        return EvaluationResult(score=similarity.numpy(force=True)[0][0])\n",
    "\n",
    "class SBERTQwen06(Evaluator):\n",
    "    name=\"SBERT Qwen3 0.6B\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = SentenceTransformer(\n",
    "            \"Qwen/Qwen3-Embedding-0.6B\", \n",
    "            similarity_fn_name=SimilarityFunction.COSINE,\n",
    "            model_kwargs={\"device_map\": \"auto\"},\n",
    "            tokenizer_kwargs={\"padding_side\": \"left\"},\n",
    "        )\n",
    "    def evaluate(self, output: str, expected, **kwargs) -> EvaluationResult:\n",
    "        rag_answer_embeddings = self.model.encode([output])\n",
    "        expected_embeddings = self.model.encode([expected[\"chatgpt\"]])\n",
    "        similarity = self.model.similarity(rag_answer_embeddings, expected_embeddings)\n",
    "        return EvaluationResult(score=similarity.numpy(force=True)[0][0])\n",
    "\n",
    "# qwen4 = SBERTQwen4()\n",
    "qwen06 = SBERTQwen06()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b249de79-043b-469d-a052-9cfb3859093a",
   "metadata": {},
   "source": [
    "# Szomszédos mondatok szemantikus távolsága"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3ddce5-4d98-4965-8786-0732ddaed8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_texts = search_pdf(ticker=\"RIO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706a4d88-5219-4be0-b5fd-d13ea0e74428",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def szemantikus_tavolsag(text: str, embeddings) -> list:\n",
    "    \"\"\"\n",
    "    Egy szöveg szomszédos mondatai közötti szemantikus távolság (1 - koszinusz-hasonlóság) kiszámítása és vizualizálása.\n",
    "\n",
    "    Ez a függvény a bemeneti szöveget mondatokra bontja, majd a mondatok beágyazásait (embeddingjeit) használva\n",
    "    kiszámítja a szomszédos mondatok közötti szemantikus távolságot. Az eredményeket hisztogram formájában ábrázolja.\n",
    "\n",
    "    Paraméterek:\n",
    "        text (str): A bemeneti szöveg, amelyet elemezni kell.\n",
    "        embeddings: Egy beágyazási (embedding) objektum, amely rendelkezik `embed_documents` metódussal.\n",
    "\n",
    "    Visszatérési érték:\n",
    "        list: A szomszédos mondatok szemantikus távolsága.\n",
    "    \"\"\"\n",
    "    \n",
    "    # a szöveg darabolása ugyanugy mint a SemanticChunker osztály teszi\n",
    "    sentences = re.split(r\"(?<=[.?!])\\s+\", text)\n",
    "\n",
    "    # egyes mondatok beágyazása\n",
    "    sentence_embeddings = embeddings.embed_documents(sentences)\n",
    "\n",
    "    # szomszédos mondatok szemantikus távolsága\n",
    "    similarities = []\n",
    "    differences = []\n",
    "    pairs = []\n",
    "    for i in range(len(sentences) - 1):\n",
    "        sim = cosine_similarity(\n",
    "            [sentence_embeddings[i]], [sentence_embeddings[i+1]]\n",
    "        )[0][0]\n",
    "        diff = 1 - sim\n",
    "        similarities.append(sim)\n",
    "        differences.append(diff)\n",
    "        pairs.append((sentences[i], sentences[i+1]))\n",
    "\n",
    "    # 7. Plot distribution of differences\n",
    "    plt.hist(differences, bins=10, edgecolor=\"black\")\n",
    "    plt.title(\"A szomszédos mondatok szemantikus távolsága\")\n",
    "    plt.xlabel(\"Távolság (1 - koszinusz-hasonlóság)\")\n",
    "    plt.ylabel(\"Frekvencia\")\n",
    "    plt.show()\n",
    "\n",
    "    return differences\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-0.6B\"\n",
    ")\n",
    "distances = szemantikus_tavolsag(df_texts['knowledge/rio_20210323_20210323_qa_1.pdf'], embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccdbda00-55da-4ee7-a912-2bb1f5a12b68",
   "metadata": {},
   "source": [
    "# Evaluate basic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14567881-dcce-42a6-b627-b99f2531a979",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phoenix.experiments.types import Example\n",
    "from basic_rag import MyRAG\n",
    "from phoenix.experiments import run_experiment\n",
    "\n",
    "\n",
    "def task(input, expected) -> str:\n",
    "    question = input['question']\n",
    "    \n",
    "    \n",
    "    # mock the RAG generation\n",
    "    rag_answer = MyRAG().invoke(question)\n",
    "    \n",
    "    return rag_answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b9d125-6cf3-4574-b980-f7b6f4e194c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_result = pd.DataFrame()\n",
    "\n",
    "result_file = \"rio_validation.json\"\n",
    "if os.path.exists(result_file):\n",
    "    df_all_result = pd.read_json(result_file)\n",
    "df_all_result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8af5e2-7c32-42d5-b4cb-027861ca693d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitting_sets = [\n",
    "     {\n",
    "        \"splitter\": \"Semantic\", \n",
    "        \"model_name\": \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        \"breakpoint_threshold_type\": \"percentile\",\n",
    "        \"breakpoint_threshold_amount\": 70.0\n",
    "    },\n",
    "    {\n",
    "        \"splitter\": \"Semantic\", \n",
    "        \"model_name\": \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        \"breakpoint_threshold_type\": \"percentile\",\n",
    "        \"breakpoint_threshold_amount\": 80.0\n",
    "    },\n",
    "    {\n",
    "        \"splitter\": \"Semantic\", \n",
    "        \"model_name\": \"Qwen/Qwen3-Embedding-0.6B\",\n",
    "        \"breakpoint_threshold_type\": \"standard_deviation\",\n",
    "        \"breakpoint_threshold_amount\": 1.25\n",
    "    },\n",
    "    {\n",
    "        \"splitter\": \"Semantic\", \n",
    "        \"model_name\": \"FinLang/finance-embeddings-investopedia\",\n",
    "        \"breakpoint_threshold_type\": \"percentile\",\n",
    "        \"breakpoint_threshold_amount\": 70.0\n",
    "    },\n",
    "    {\n",
    "        \"splitter\": \"Semantic\", \n",
    "        \"model_name\": \"FinLang/finance-embeddings-investopedia\",\n",
    "        \"breakpoint_threshold_type\": \"standard_deviation\",\n",
    "        \"breakpoint_threshold_amount\": 1.25\n",
    "    },\n",
    "    {\n",
    "        \"splitter\": \"Agentic\",\n",
    "        \"max_content\":300,\n",
    "        \"markdown_presplit\": False\n",
    "    },\n",
    "    {\n",
    "        \"splitter\": \"Agentic\",\n",
    "        \"max_content\":60,\n",
    "        \"markdown_presplit\": False\n",
    "    },\n",
    "    {\n",
    "        \"splitter\": \"Agentic\",\n",
    "        \"max_content\":200,\n",
    "        \"markdown_presplit\": False\n",
    "    },\n",
    "    {\n",
    "        \"splitter\": \"RecursiveCharacter\",\n",
    "        \"chunk_overlap\": 0,\n",
    "        \"chunk_size\": 1000\n",
    "    },\n",
    "        {\n",
    "        \"splitter\": \"RecursiveCharacter\",\n",
    "        \"chunk_overlap\": 300,\n",
    "        \"chunk_size\": 3000\n",
    "    },\n",
    "    {\n",
    "        \"splitter\": \"Semantic\", \n",
    "        \"model_name\": \"FinLang/finance-embeddings-investopedia\",\n",
    "        \"breakpoint_threshold_type\": \"percentile\",\n",
    "        \"breakpoint_threshold_amount\": 70.0,\n",
    "        \"markdown_presplit\": False\n",
    "    },\n",
    "    {\n",
    "        \"splitter\": \"Semantic\", \n",
    "        \"model_name\": \"FinLang/finance-embeddings-investopedia\",\n",
    "        \"breakpoint_threshold_type\": \"standard_deviation\",\n",
    "        \"breakpoint_threshold_amount\": 1.25,\n",
    "        \"markdown_presplit\": False\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15721eb3-64c1-4507-a95a-10d0362a91d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter out settings which was tested before\n",
    "filtered_splitting_sets = []\n",
    "for splitting_setting in splitting_sets:\n",
    "    if df_all_result.shape[0] == 0:\n",
    "        filtered_splitting_sets.append(splitting_setting)\n",
    "    elif str(splitting_setting) not in set(df_all_result[\"setting\"]):\n",
    "        filtered_splitting_sets.append(splitting_setting)\n",
    "print(filtered_splitting_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251bc8bd-f8a2-4e2f-924b-193bbd9d66d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the different experiments\n",
    "for splitting_setting in filtered_splitting_sets:\n",
    "    start_timestamp = datetime.datetime.now()\n",
    "    \n",
    "    paragraphs = get_paragraphs(\n",
    "        ticker=\"RIO\",\n",
    "        max_lenght=200,\n",
    "        **splitting_setting  # Unpack the rest of the settings dynamically\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        knowledge = MilvusKnowledgeStorage()\n",
    "    except:\n",
    "        # retry\n",
    "        sleep(1)\n",
    "        knowledge = MilvusKnowledgeStorage()\n",
    "    knowledge.initialize_knowledge_storage()\n",
    "\n",
    "    # load data\n",
    "    documents = [ doc.page_content for doc in paragraphs]\n",
    "    metadata = [ doc.metadata for doc in paragraphs]\n",
    "    knowledge.save(documents=documents, metadata=metadata)\n",
    "\n",
    "    data_load_timestamp = datetime.datetime.now()\n",
    "\n",
    "    experiment = run_experiment(\n",
    "        dataset_best,\n",
    "        task,\n",
    "        experiment_name=\"rag-experiment\",\n",
    "        evaluators=[qwen06, BERTScore() ],\n",
    "        experiment_metadata=splitting_setting\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        evaluation_result = experiment.get_evaluations()\n",
    "    except:\n",
    "        evaluation_result = experiment.get_evaluations()\n",
    "\n",
    "    evaluation_result_filtered = pd.DataFrame({\n",
    "        \"name\": evaluation_result[\"name\"].values,\n",
    "        \"score\": evaluation_result[\"score\"].values,\n",
    "        \"setting\": str(splitting_setting),\n",
    "        \"data_load_time\": (data_load_timestamp-start_timestamp).total_seconds()\n",
    "    })\n",
    "\n",
    "    df_all_result = pd.concat([df_all_result, evaluation_result_filtered])\n",
    "    df_all_result.to_json(result_file, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f131e9e-bb8f-468e-bc06-8cbfe02d527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as plotly_express\n",
    "import plotly.graph_objects as plotly_go\n",
    "\n",
    "custom_template = {\n",
    "    \"layout\": plotly_go.Layout(\n",
    "        font={\n",
    "            \"family\": \"Nunito\",\n",
    "            \"size\": 12,\n",
    "            \"color\": \"#707070\",\n",
    "        },\n",
    "        title={\n",
    "            \"font\": {\n",
    "                \"family\": \"Lato\",\n",
    "                \"size\": 18,\n",
    "                \"color\": \"#1f1f1f\",\n",
    "            },\n",
    "        },\n",
    "        plot_bgcolor=\"#ffffff\",\n",
    "        paper_bgcolor=\"#ffffff\",\n",
    "        colorway=plotly_express.colors.qualitative.G10,\n",
    "    )\n",
    "}\n",
    "\n",
    "def format_title(title, subtitle=None, subtitle_font_size=14):\n",
    "    title = f'<b>{title}</b>'\n",
    "    if not subtitle:\n",
    "        return title\n",
    "    subtitle = f'<span style=\"font-size: {subtitle_font_size}px;\">{subtitle}</span>'\n",
    "    return f'{title}<br>{subtitle}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b343fa06-778f-457d-85b1-9cc5a5cfae81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_result.sort_values([\"name\", \"setting\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5327d5d-79dd-4003-bcc7-fa2ac3d4fc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a boxplot using Plotly\n",
    "fig = plotly_express.box(\n",
    "    df_all_result,\n",
    "    x=\"name\",\n",
    "    y=\"score\",\n",
    "    color=\"setting\",\n",
    "    title=format_title(\"Szöveg darabolása\", \"RAG megvalósítás teljesítménye\"),\n",
    "    labels={\"name\": \"Evaluation Type\", \"score\": \"Similarity Score\",  \"setting\": \"Setting\"},\n",
    "    template=custom_template\n",
    ")\n",
    "\n",
    "# Customize the layout\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Mérési módszer\",\n",
    "    yaxis_title=\"Hasonlóság\",\n",
    "    xaxis=dict(tickangle=45),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "# save\n",
    "fig.write_html(result_file.replace(\".json\", \".html\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8c5e05-e405-4caf-bc2f-e1e8a475b099",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_result.to_json(result_file, orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05daf09f-9b02-48de-b8fc-e9b3212cc58e",
   "metadata": {},
   "source": [
    "# Cleaning up\n",
    "\n",
    "To cleanup the unnecessary HuggingFace models run the following command: huggingface-cli delete-cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4473ef4e-da39-40e7-b2e3-1660abedd476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
