{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "825416b1-f13c-42a8-ae1b-065c611ade98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q --upgrade langchain langchain-experimental langchain-openai python-dotenv pyvis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c55d4b2-afc7-4c37-84b6-2f9715c10f81",
   "metadata": {},
   "source": [
    "# Set up Phoenix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5649ed-f1d6-4da6-8557-647979b4cd49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding of current TracerProvider is not allowed\n",
      "Attempting to instrument while already instrumented\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî≠ OpenTelemetry Tracing Details üî≠\n",
      "|  Phoenix Project: Knowledge_base_QA\n",
      "|  Span Processor: SimpleSpanProcessor\n",
      "|  Collector Endpoint: https://app.phoenix.arize.com/v1/traces\n",
      "|  Transport: HTTP + protobuf\n",
      "|  Transport Headers: {'api_key': '****'}\n",
      "|  \n",
      "|  Using a default SpanProcessor. `add_span_processor` will overwrite this default.\n",
      "|  \n",
      "|  ‚ö†Ô∏è WARNING: It is strongly advised to use a BatchSpanProcessor in production environments.\n",
      "|  \n",
      "|  `register` has set this TracerProvider as the global OpenTelemetry default.\n",
      "|  To disable this behavior, call `register` with `set_global_tracer_provider=False`.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from phoenix.otel import register\n",
    "from openinference.instrumentation.openai import OpenAIInstrumentor\n",
    "import os\n",
    "from opentelemetry.trace import Status, StatusCode\n",
    "from openinference.semconv.trace import SpanAttributes\n",
    "\n",
    "project_name = \"Knowledge_base_QA\"\n",
    "\n",
    "# Add Phoenix API Key for tracing\n",
    "phoenix_key = ''\n",
    "with open('phoenix_key.txt', 'r') as file:\n",
    "    phoenix_key = file.read()\n",
    "os.environ[\"PHOENIX_CLIENT_HEADERS\"] = f\"api_key={phoenix_key}\"\n",
    "os.environ[\"PHOENIX_COLLECTOR_ENDPOINT\"] = \"https://app.phoenix.arize.com\"\n",
    "os.environ[\"OTEL_EXPORTER_OTLP_HEADERS\"] = f\"api_key={phoenix_key}\";\n",
    "os.environ['PHOENIX_PROJECT_NAME'] = project_name\n",
    "\n",
    "# configure the Phoenix tracer\n",
    "tracer_provider = register(\n",
    "  project_name=project_name, # Default is 'default'\n",
    "  auto_instrument=True # Auto-instrument your app based on installed OI dependencies\n",
    ")\n",
    "\n",
    "OpenAIInstrumentor().instrument(tracer_provider = tracer_provider)\n",
    "tracer = tracer_provider.get_tracer(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837ba17e-a050-4d64-8169-2c4abf63950c",
   "metadata": {},
   "source": [
    "# Basic imports and setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a53cea51-9724-48ca-869c-cc3214c10792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import datetime\n",
    "import json\n",
    "from time import sleep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field\n",
    "from pybars import Compiler\n",
    "import yaml\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "from mlx_lm import load, generate\n",
    "\n",
    "# import own utility functions\n",
    "from llm_graph import *\n",
    "from pdf_preprocessing import *\n",
    "\n",
    "\n",
    "# settings\n",
    "model_type = \"openrouter\"\n",
    "ticker = 'BHP'\n",
    "ticker_profile = \"BHP Group Limited operates as a resources company in Australia, Europe, China, Japan, India, South Korea, the rest of Asia, North America, South America, and internationally. The company operates through Copper, Iron Ore, and Coal segments. It engages in the mining of copper, uranium, gold, zinc, lead, molybdenum, silver, iron ore, cobalt, and metallurgical and energy coal. The company is also involved in the mining, smelting, and refining of nickel, as well as potash development activities. In addition, it provides towing, freight, marketing and trading, marketing support, finance, administrative, and other services. The company was founded in 1851 and is headquartered in Melbourne, Australia.\"\n",
    "version = '1.6.0'\n",
    "\n",
    "# Cut the input text to paragraph, if False it will cut to PDF pages\n",
    "cut_in_paragraph = False\n",
    "template_file_path = './prompts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813e5ff-e1de-410b-aa79-9eddec0b8080",
   "metadata": {},
   "source": [
    "# LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c517b5e8-a5ea-4ae4-ab1f-cc640a4502e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use MLX_LLM in the background\n",
    "\n",
    "if model_type == \"mlx\":\n",
    "    # local models\n",
    "    model_name = 'mlx-community/Meta-Llama-3.1-8B-Instruct-4bit'\n",
    "    # 8 bit cab fit in M4 memmory but seems the 4bit enough for our task\n",
    "    # so do not justfly the double memory usage\n",
    "    # model_name = 'mlx-community/Meta-Llama-3.1-8B-Instruct-8bit')\n",
    "    # model_name = 'mlx-community/Qwen3-8B-6bit'\n",
    "    # model_name = 'mlx-community/gemma-3-12b-it-4bit-DWQ')\n",
    "\n",
    "    api_key = \"nem_kell\"\n",
    "\n",
    "    base_url = \"http://localhost:8000/v1\"\n",
    "elif model_type == \"openrouter\":\n",
    "    # openrouter models\n",
    "    model_name = \"qwen/qwen3-30b-a3b:free\"\n",
    "\n",
    "    with open('openrouter_key.txt', 'r') as file:\n",
    "        api_key = file.read()\n",
    "\n",
    "    base_url = \"https://openrouter.ai/api/v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ed1bb47-7eb6-4947-806b-15a83586cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(\n",
    "    temperature=0, \n",
    "    model_name=model_name, \n",
    "    openai_api_base=base_url,\n",
    "    api_key=api_key\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a8b52-514b-4ae7-8081-eff425bdbbc7",
   "metadata": {},
   "source": [
    "# Read data\n",
    "\n",
    "Read PDF financial reports and process them to Langchain Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99221d3a-e750-4786-84bc-ffeb569e1b17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: knowledge/bhp_20240701_20241231_qa_1.pdf file\n"
     ]
    }
   ],
   "source": [
    "paragraphs = get_paragraphs(ticker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2481bc13-c4e0-4159-94ca-50eb02042145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load qa_prompt\n",
    "# open file\n",
    "system_propmt_file = os.path.join(template_file_path, 'qa_classification.yaml')\n",
    "# test file exist\n",
    "if not os.path.isfile(system_propmt_file):\n",
    "    raise ValueError(f\"{system_propmt_file} not a valid file\")\n",
    "with open(system_propmt_file) as file:\n",
    "    source = yaml.safe_load(file)\n",
    "# Compile the template\n",
    "compiler = Compiler()\n",
    "qa_prompt_template = compiler.compile(source['prompt_template'])\n",
    "qa_propmt = qa_prompt_template({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "027acc42-7b2c-4976-9f62-74e109c6acef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0%"
     ]
    }
   ],
   "source": [
    "# we will use this for quality testing in Phoenix\n",
    "data_sample = []\n",
    "\n",
    "# itterate over the paragrpahs and decide what category they are\n",
    "stepsize = 3\n",
    "with tracer.start_as_current_span(f\"qa_category_{datetime.datetime.now().timestamp()}\") as parent_span: \n",
    "    for pidx in range(stepsize, len(paragraphs)+1):\n",
    "        print(f\"Progress: {np.round(100*pidx/len(paragraphs),2)}%\", end=\"\\r\")\n",
    "        this_section = paragraphs[pidx-stepsize:pidx]\n",
    "        text_json = {\n",
    "            \"COMPANY\": ticker,\n",
    "            \"COMPANY_DESCRIPTION\": ticker_profile\n",
    "        }\n",
    "        for p in range(len(this_section)):\n",
    "            if \"qa_type\" not in this_section[p].metadata:\n",
    "                this_section[p].metadata['qa_type'] = []\n",
    "            text_json[f\"text_{p}\"] = this_section[p].page_content\n",
    "        data_sample.append(text_json)\n",
    "    \n",
    "        messages = [\n",
    "            (\"system\", qa_propmt),\n",
    "            (\"user\", json.dumps(text_json, indent=4))\n",
    "        ]\n",
    "\n",
    "        # Phoenix span\n",
    "        with tracer.start_as_current_span(\"child\") as child_span:\n",
    "            child_span.set_attribute(SpanAttributes.INPUT_VALUE, json.dumps(text_json, indent=4))\n",
    "            child_span.set_attribute(SpanAttributes.LLM_MODEL_NAME, model_name)\n",
    "\n",
    "            try:\n",
    "                response = llm.with_structured_output(QAType).invoke(messages)\n",
    "            except Exception as e:\n",
    "                child_span.set_attribute(SpanAttributes.OUTPUT_VALUE, str(e))\n",
    "                child_span.set_status(Status(StatusCode.ERROR))\n",
    "                continue\n",
    "\n",
    "            response = dict(response)\n",
    "            child_span.set_attribute(SpanAttributes.OUTPUT_VALUE, json.dumps(response, indent=4))\n",
    "\n",
    "            for p in range(len(this_section)):\n",
    "                this_section[p].metadata['qa_type'].append(response[f\"text_{p+1}_class\"])\n",
    "\n",
    "            child_span.set_status(Status(StatusCode.OK))\n",
    "    # we use a free model so there is limited \n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f16eae-1a23-4c40-a72f-c8b472912a98",
   "metadata": {},
   "source": [
    "# Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb0c5c8-f330-45df-ba41-afcd195bb634",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "filtered_paragraphs = []\n",
    "\n",
    "for pidx in range(len(paragraphs)):\n",
    "    # clean QA category\n",
    "    qa_category, counts = np.unique(paragraphs[pidx].metadata['qa_type'], return_counts=True)\n",
    "\n",
    "    qa_category = { \n",
    "        str(category): int(count) \n",
    "        for category, count in zip(qa_category, counts)\n",
    "        if str(category) != \"NONE_OF_ABOVE\"\n",
    "    }\n",
    "\n",
    "    if len(qa_category.values()) == 1:\n",
    "        this_paragraphs = paragraphs[pidx].model_copy()\n",
    "        this_paragraphs.metadata['qa_type'] = list(qa_category.keys())[0]\n",
    "        filtered_paragraphs.append(this_paragraphs)\n",
    "\n",
    "print(len(filtered_paragraphs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "17855c95-57d3-4002-9acd-88d03e58dd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress: 100.0%"
     ]
    }
   ],
   "source": [
    "from llm_graph import *\n",
    "\n",
    "# Define the allowed node schemas\n",
    "node_schemas = [\n",
    "    NodeSchema(\n",
    "        \"Person\",\n",
    "        [\"name\", \"birth_year\", \"profession\"],\n",
    "        \"Represents an individual human being. Examples include employees, entrepreneurs, or public figures.\"\n",
    "    ),\n",
    "    NodeSchema(\n",
    "        \"Company\",\n",
    "        [\"name\"],\n",
    "        \"Represents a business entity or organization. This includes attributes such as the company's name and the industry it operates in (e.g., finance, technology, manufacturing). Examples include multinational corporations, startups, or small businesses.\"\n",
    "    ),\n",
    "    NodeSchema(\n",
    "        \"Product\",\n",
    "        [\"name\"],\n",
    "        \"Represents a tangible or intangible item that is created, manufactured, or offered by a company. This includes goods such as electronics, clothing, or software. A Product's name should always be a noun.\"\n",
    "    ),\n",
    "    NodeSchema(\n",
    "        \"Activity\",\n",
    "        [\"name\"],\n",
    "        \"Represents a specific business activity or operation performed by a company. This could include manufacturing processes, service offerings, or other business-related activities such as acquisitions, partnerships, or marketing campaigns. Examples include 'Car Manufacturing', 'Software Developing', or 'Market Expansion'. An Activity's name should always be a verb.\"\n",
    "    ),\n",
    "    NodeSchema(\n",
    "        \"Location\",\n",
    "        [\"name\"],\n",
    "        \"Represents a geographical area, such as a city, country, or region. This includes physical locations where companies operate, products are manufactured, or activities take place. Examples include 'New York City', 'Germany', or 'Asia-Pacific Region'.\"\n",
    "    ),\n",
    "]\n",
    "    \n",
    "# Define the allowed relationship schemas\n",
    "relationship_schemas = [\n",
    "    RelationshipSchema(\"Person\", \"WORK_FOR\", \"Company\", [\"start_year\", \"end_year\", \"year\"]),\n",
    "    RelationshipSchema(\"Company\", \"OPERATE_IN\", \"Location\"),\n",
    "    RelationshipSchema(\"Company\", \"MADE_BY\", \"Product\", [\"start_year\", \"end_year\", \"year\"]),\n",
    "    RelationshipSchema(\"Company\", \"DOING\", \"Activity\", [\"start_year\", \"end_year\", \"year\"]),\n",
    "]\n",
    "\n",
    "additional_instructions = f\"\"\"\n",
    "# Background information about the text\n",
    "\n",
    "The text is from the {ticker} company investor call transcript. {ticker_profile}. \n",
    "In the transcript investors asking question from the {ticker} representatives who answers them.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Generate the Graph class\n",
    "Graph = generate_graph_class(node_schemas.copy(), relationship_schemas.copy())\n",
    "\n",
    "# Load prompt template\n",
    "node_definitions = format_node_schemas(node_schemas)\n",
    "node_properties_definitions = format_node_properties_schemas(node_schemas)\n",
    "relationship_definitions = format_relationship_schemas(relationship_schemas)\n",
    "relationship_properties_definitions = format_relationship_properties_schemas(relationship_schemas)\n",
    "compiler = Compiler()\n",
    "# open file\n",
    "system_propmt_file = os.path.join(template_file_path, 'knowledge_graph.yaml')\n",
    "# test file exist\n",
    "if not os.path.isfile(system_propmt_file):\n",
    "    raise ValueError(f\"{system_propmt_file} not a valid file\")\n",
    "with open(system_propmt_file) as file:\n",
    "    source = yaml.safe_load(file)\n",
    "# Compile the template\n",
    "knowledge_graph_template = compiler.compile(source['prompt_template'])\n",
    "\n",
    "graph_documents_nodes_defined = []\n",
    "\n",
    "with tracer.start_as_current_span(f\"graphs_{datetime.datetime.now().timestamp()}\") as parent_span: \n",
    "    for pidx in range(len(filtered_paragraphs)):\n",
    "        print(f\"Progress: {np.round(100*(pidx+1)/len(filtered_paragraphs),2)}%\", end=\"\\r\")\n",
    "        paragraph = filtered_paragraphs[pidx]\n",
    "        formatted_prompt = knowledge_graph_template({\n",
    "            \"node_definitions\": node_definitions,\n",
    "            \"relationship_definitions\": relationship_definitions,\n",
    "            \"node_properties_definitions\": node_properties_definitions,\n",
    "            \"relationship_properties_definitions\": relationship_properties_definitions,\n",
    "            \"additional_instructions\": additional_instructions,\n",
    "            \"input_text\": paragraph.page_content\n",
    "        })\n",
    "\n",
    "        messages = [\n",
    "            (\"user\", formatted_prompt)\n",
    "        ]\n",
    "\n",
    "        with tracer.start_as_current_span(\"child\") as child_span:\n",
    "            child_span.set_attribute(SpanAttributes.INPUT_VALUE, formatted_prompt)\n",
    "            child_span.set_attribute(SpanAttributes.LLM_MODEL_NAME, model_name)\n",
    "            try:\n",
    "                result = llm.with_structured_output(Graph).invoke(messages)\n",
    "            except Exception as e:\n",
    "                child_span.set_attribute(SpanAttributes.OUTPUT_VALUE, str(e))\n",
    "                child_span.set_status(Status(StatusCode.ERROR))\n",
    "                continue\n",
    "\n",
    "            enriched_graph = add_paragraph_node(\n",
    "                result,\n",
    "                paragraph.page_content, \n",
    "                title=paragraph.metadata[\"filename\"], \n",
    "                filename=paragraph.metadata[\"filename\"]\n",
    "            )\n",
    "            graph_documents_nodes_defined.append(enriched_graph)\n",
    "            child_span.set_attribute(SpanAttributes.OUTPUT_VALUE, json.dumps(result.model_dump(), indent=4))\n",
    "            child_span.set_status(Status(StatusCode.OK))\n",
    "    \n",
    "        sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fbe6f863-0db2-405b-8488-ac1b8d2160c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the list of Graph together\n",
    "merged_graph_documents_nodes_defined = merge_graphs(graph_documents_nodes_defined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "816214e2-daab-4f92-9939-066a2d6b8459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'europe'\n",
      "'china'\n",
      "'japan'\n",
      "'india'\n",
      "'south korea'\n",
      "'rest of asia'\n",
      "'north america'\n",
      "'south america'\n",
      "'international'\n",
      "'copper'\n",
      "'iron ore'\n",
      "'coal'\n",
      "'uranium'\n",
      "'gold'\n",
      "'zinc'\n",
      "'lead'\n",
      "'molybdenum'\n",
      "'silver'\n",
      "'cobalt'\n",
      "'metallurgical and energy coal'\n"
     ]
    }
   ],
   "source": [
    "# clean up the nodes\n",
    "clean_graph = merge_nodes(merged_graph_documents_nodes_defined)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417e99d-5a43-496f-b8fe-595710661ce7",
   "metadata": {},
   "source": [
    "# Visualization\n",
    "\n",
    "For the visualization result check: [Github page](https://sajozsattila.github.io/blog/Knowledge_base/knowledge_graph.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b312bb01-ce33-4085-97c8-b22d50dda4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2272dc33-61d9-492b-8654-f023349b5397",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph saved to /Users/Attila_Sajo/Git/blog/Knowledge_base/knowledge_graph.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<class 'pyvis.network.Network'> |N|=92 |E|=231"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualize_graph(clean_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7179e68-8fad-4273-b116-6b56379ef2d8",
   "metadata": {},
   "source": [
    "# Load to Kuzu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "05fddcde-53ba-4ac1-9e43-5c63ed901a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import kuzu\n",
    "import hashlib\n",
    "\n",
    "# import polars as pl\n",
    "\n",
    "db = kuzu.Database(\":memory:\")\n",
    "db = kuzu.Database()\n",
    "conn = kuzu.Connection(db)\n",
    "\n",
    "def load_to_kuzu(graph: Graph):\n",
    "    nodes = graph.nodes\n",
    "    relations = graph.relationships\n",
    "\n",
    "    # get unique node types\n",
    "    node_types = set([ node.type for node in nodes ])\n",
    "    # itterate over node types and create individual DataFrames for load\n",
    "    for node_type in node_types:\n",
    "        # filter the nodes\n",
    "        this_nodes = [ node for node in nodes if node.type == node_type ]\n",
    "        # detect primery key\n",
    "        primary_key = detect_primary_key(this_nodes)\n",
    "        # the SQL to create the node table\n",
    "        create_node_table_sql = f\"CREATE NODE TABLE IF NOT EXISTS {node_type}(\\n\"\n",
    "        # get attibutes\n",
    "        attirbutes = [ list(node.properties.keys()) for node in this_nodes ]\n",
    "        attirbutes = set([item for sublist in attirbutes for item in sublist])\n",
    "        df = pd.DataFrame()\n",
    "        for attirbute in attirbutes:\n",
    "            this_attribute = []\n",
    "            for node in this_nodes:\n",
    "                if attirbute in list(node.properties.keys()):\n",
    "                    this_attribute.append(node.properties[attirbute])\n",
    "                else:\n",
    "                    this_attribute.append(None)\n",
    "            df[attirbute] = this_attribute\n",
    "        # get SQL type from Pandas dtype\n",
    "        attributes_types = pandas_to_sql_types(df)\n",
    "        # add attribute columns to SQL table\n",
    "        for k,v in attributes_types.items():\n",
    "            create_node_table_sql += f\"    {k} {v},\\n\"\n",
    "        # if id is the primary key\n",
    "        if primary_key[\"primary_key\"] is None:\n",
    "            df['id'] = [ hashlib.md5(str(node.id).encode()).hexdigest() for node in this_nodes ]\n",
    "        else:\n",
    "            df['id'] = [ \n",
    "                hashlib.md5(str(node.properties[primary_key[\"primary_key\"][0]]).encode()).hexdigest() \n",
    "                for node in this_nodes \n",
    "            ]\n",
    "        # close the SQL defination for Node table\n",
    "        create_node_table_sql += \"    id STRING PRIMARY KEY\\n);\"\n",
    "\n",
    "        # create node table\n",
    "        conn.execute(create_node_table_sql)\n",
    "        # load nodes\n",
    "        conn.execute(f\"COPY {node_type} FROM $dataframe\", {\"dataframe\": df})\n",
    "\n",
    "    # define relations in the DB\n",
    "    # get unique relation types\n",
    "    relation_types = set([ relation.type for relation in relations ])\n",
    "    for relation_type in relation_types:\n",
    "        # filter the nodes\n",
    "        this_relations = [ relation for relation in relations if relation.type == relation_type ]\n",
    "        # get the relationships where the from and to node types are same\n",
    "        connections_directions = [ {'source': relation.source_type, 'target': relation.target_type } for relation in this_relations ]\n",
    "        connections_directions = [dict(t) for t in {tuple(d.items()) for d in connections_directions}]\n",
    "        for connections_direction in connections_directions:\n",
    "            table_name = f'{relation_type}_{connections_direction[\"source\"].upper()}_{connections_direction[\"target\"].upper()}'\n",
    "            create_relation_table_sql = f'CREATE REL TABLE IF NOT EXISTS {table_name}(\\n'\n",
    "            create_relation_table_sql += f'    FROM {connections_direction[\"source\"]} TO {connections_direction[\"target\"]}\\n'\n",
    "            create_relation_table_sql += \");\"\n",
    "            conn.execute(create_relation_table_sql)\n",
    "\n",
    "            # get the nodes for this df\n",
    "            this_table_relations = [ \n",
    "                relation for relation in this_relations\n",
    "                if relation.source_type == connections_direction[\"source\"]\n",
    "                and relation.target_type == connections_direction[\"target\"]\n",
    "            ]\n",
    "            df = pd.DataFrame({\n",
    "                \"from\": [ hashlib.md5(str(relation.source).encode()).hexdigest() for relation in this_table_relations ],\n",
    "                \"to\": [ hashlib.md5(str(relation.target).encode()).hexdigest() for relation in this_table_relations ]\n",
    "            })\n",
    "            load_relation_table_sql = f\"\"\"\n",
    "                COPY {table_name} FROM $dataframe (ignore_errors=true)\n",
    "            \"\"\"\n",
    "            conn.execute(load_relation_table_sql, {\"dataframe\": df})\n",
    "        \n",
    "    \n",
    "load_to_kuzu(clean_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c8451cb-5d3b-4494-a66a-a2fce8a72315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               p.id             p.name             c.name  \\\n",
      "0  f05db2fb41cfacf96466b4d301672366   Liam Fitzpatrick      Deutsche Bank   \n",
      "1  1d6d7c8e66cfd9f706ab094dc130df30         Mike Henry  BHP Group Limited   \n",
      "2  e48a55f3e88d2e94dc9dbb50ef3f4b86   Jason Fairclough    Bank of America   \n",
      "3  7348b4c64e83411dde284c168d5a908b      Amos Fletcher  BHP Group Limited   \n",
      "4  20efa746983df86c0c36eeac16a5fb9d       Vandita Pant  BHP Group Limited   \n",
      "5  4f1d6f45e3eb82dbaf454aeb46483ab4  VANDITA PANT, BHP  BHP Group Limited   \n",
      "6  1bddabaffa161cace10d4e9ad6527d70        Matt Greene  BHP Group Limited   \n",
      "\n",
      "                               c.id  \n",
      "0  54ebc4b3f1a2a0984d8e6fe04e5b873d  \n",
      "1  f458659825cb5b9141b09d1e658f6132  \n",
      "2  e4b68a3710443456cf882f506dfa2af5  \n",
      "3  f458659825cb5b9141b09d1e658f6132  \n",
      "4  f458659825cb5b9141b09d1e658f6132  \n",
      "5  f458659825cb5b9141b09d1e658f6132  \n",
      "6  f458659825cb5b9141b09d1e658f6132  \n"
     ]
    }
   ],
   "source": [
    "rows = conn.execute(\"MATCH (p:Person)-[:WORK_FOR_PERSON_COMPANY]->(c:Company) RETURN p.id, p.name, c.name, c.id\")\n",
    "print(rows.get_as_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1267142d-82df-4109-8899-e08f2e3a532d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              c.name                              c.id\n",
      "0  BHP Group Limited  f458659825cb5b9141b09d1e658f6132\n",
      "1      Deutsche Bank  54ebc4b3f1a2a0984d8e6fe04e5b873d\n",
      "2             Vicu√±a  4d2705c724cfc91caa9dc64119d8456e\n",
      "3    Bank of America  e4b68a3710443456cf882f506dfa2af5\n",
      "4           Barclays  167cd5762065530b3bf05b9d1f4fed66\n",
      "5          Westshore  9f3c120af482df44adf99fb395e7471c\n",
      "6      Goldman Sachs  d81a3a023d342920551254d6f821d7b2\n"
     ]
    }
   ],
   "source": [
    "rows = conn.execute(\"MATCH (c:Company) RETURN c.name, c.id\")\n",
    "print(rows.get_as_df())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6caad9f-0353-4b3b-871a-17093fdec9c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Node(id='BHP Group Limited', type='Company', properties={'name': 'BHP Group Limited'}),\n",
       " Node(id='bhp group limited', type='Company', properties={'name': 'BHP Group Limited'}),\n",
       " Node(id='bhp', type='Company', properties={'name': 'BHP Group Limited'})]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ node for node in merged_graph_documents_nodes_defined.nodes if \"name\" in node.properties and node.properties[\"name\"] == \"BHP Group Limited\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2da315-5043-4fb3-a4c8-f30532023615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
